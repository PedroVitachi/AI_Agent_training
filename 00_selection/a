# üìä Challenge ‚Äì Data Science (J√∫nior)

Bem-vindo ao **Challenge de Data Science** da BU de Dados e IA da GFT!

Este desafio faz parte do nosso processo de sele√ß√£o para integrar a equipe respons√°vel pelo **Agent Factory**, uma iniciativa estrat√©gica com foco em solu√ß√µes de intelig√™ncia baseadas em dados.

---

## üéØ Objetivo

Seu objetivo neste desafio √© construir um modelo simples de **machine learning** utilizando dados reais do ENEM, com o foco em **prever a nota da reda√ß√£o** de um candidato a partir de informa√ß√µes dispon√≠veis no conjunto de dados.

---

## üîç O que voc√™ vai fazer

Voc√™ ser√° respons√°vel por seguir as principais etapas de um projeto de Ci√™ncia de Dados, de forma simples e organizada:

### 1. **Carregamento e explora√ß√£o dos dados**

* Utilize o dataset que disponibilizamos neste link:
* Fa√ßa a leitura, entenda as colunas dispon√≠veis e identifique quais podem ser √∫teis para a tarefa.

### 2. **An√°lise explorat√≥ria dos dados (EDA)**

* Crie visualiza√ß√µes e tabelas para entender o comportamento das vari√°veis.
* Observe poss√≠veis correla√ß√µes com a nota da reda√ß√£o (`NU_NOTA_REDACAO`).
* Verifique a exist√™ncia de valores ausentes ou inconsistentes.

### 3. **Constru√ß√£o do modelo**

Voc√™ pode escolher entre duas abordagens:

* **Regress√£o:** prever a nota exata da reda√ß√£o (de 0 a 1000).
* **Classifica√ß√£o:** prever a faixa de desempenho da reda√ß√£o (por exemplo):

  * **Baixa**: 0‚Äì400
  * **M√©dia**: 401‚Äì800
  * **Alta**: 801‚Äì1000

> Lembre-se principalmente da etapa de Feature Engineering. O dataset tem algumas vari√°veis que, apesar de representadas por valores num√©ricos, s√£o **categ√≥ricas**. Voc√™ dever√° trat√°-las corretamente para adicion√°-las ao modelo, se julgar necess√°rio.

### 4. **Avalia√ß√£o do modelo**

* Para regress√£o: use m√©tricas como **MAE**, **RMSE** ou **R¬≤**.
* Para classifica√ß√£o: utilize **acur√°cia**, **precis√£o**, **recall**, **matriz de confus√£o**, etc.

### 5. **Conclus√£o**

* Destaque as vari√°veis que mais influenciaram na previs√£o.
* Comente o desempenho do modelo e poss√≠veis melhorias.
* Aqui o que mais vale √© sua explica√ß√£o. Demonstre que entendeu todo o processo.

---

## üìÅ Sobre os dados

Voc√™ estar√° trabalhando com um dataset real do ENEM 2019, extra√≠do do site do INEP. Para este desafio, selecionamos apenas 10.000 registros e algumas colunas, com o objetivo de n√£o tornar o dataset t√£o pesado para as an√°lises. As colunas selecionadas foram:

| Coluna            | Descri√ß√£o                                                                 | Categorias / Valores Poss√≠veis                                                       |
|-------------------|---------------------------------------------------------------------------|----------------------------------------------------------------------------------------|
| NU_ANO            | Ano de realiza√ß√£o do exame.                                               | Ex: 2019                                                                  |
| NU_NOTA_CN        | Nota em Ci√™ncias da Natureza.                                             | Varia entre 0 e 1000                                                                  |
| NU_NOTA_CH        | Nota em Ci√™ncias Humanas.                                                 | Varia entre 0 e 1000                                                                  |
| NU_NOTA_LC        | Nota em Linguagens e C√≥digos.                                             | Varia entre 0 e 1000                                                                  |
| NU_NOTA_MT        | Nota em Matem√°tica.                                                       | Varia entre 0 e 1000                                                                  |
| TP_ESCOLA         | Tipo de escola em que o participante concluiu ou est√° concluindo o EM.    | 1: N√£o respondeu <br>2: P√∫blica<br>3: Privada<br>4: Exterior       |
| NU_NOTA_REDACAO   | Nota obtida na reda√ß√£o.                                                   | Varia entre 0 e 1000                                                                  |
| TP_ST_CONCLUSAO   | Situa√ß√£o de conclus√£o do Ensino M√©dio do participante.                    | 1: J√° conclu√≠<br>2: Estou cursando e vou concluir em 2019<br>3: Estou cursando e vou concluir ap√≥s 2019 <br>4: N√£o conclu√≠ e n√£o estou cursando o Ensino M√©dio|

---

## üìå Entrega

Voc√™ dever√° entregar um **notebook Jupyter (.ipynb)** com:

* As an√°lises e visualiza√ß√µes realizadas
* O(s) modelo(s) criados
* M√©tricas utilizadas para avalia√ß√£o
* Conclus√µes finais

> Organiza√ß√£o, clareza nos coment√°rios e boas pr√°ticas contam muitos pontos!

---

## üí° Dicas

* Comece com uma amostra menor para explorar os dados mais rapidamente.
* Elimine colunas que n√£o fazem sentido (como CPF, nome, c√≥digo da prova, etc.).
* Tente diferentes algoritmos, mas priorize o entendimento sobre a complexidade.
* Comente cada etapa no seu notebook explicando o que est√° fazendo e porqu√™.
* **N√£o se preocupe *tanto* com o resultado final**. O objetivo maior do exerc√≠cio √© avaliar a linha de racioc√≠nio e o pensamento cr√≠tico resolver o case.
* Voc√™ pode usar aplica√ß√µes de IA Generativa. No entanto, todas as decis√µes ser√£o questionadas. √â importante entender o que est√° fazendo a todo momento.

---

## ‚úÖ Crit√©rios de Avalia√ß√£o

* Clareza e organiza√ß√£o do notebook
* Qualidade da an√°lise explorat√≥ria
* Adequa√ß√£o do modelo e interpreta√ß√£o dos resultados
* Justificativa das decis√µes t√©cnicas
* Comunica√ß√£o das conclus√µes

---

Boa sorte, e divirta-se no processo!
Estamos ansiosos para ver como voc√™ vai transformar dados em conhecimento. üöÄ
# 0. Imports

Fa√ßa aqui nesta se√ß√£o todos os imports que voc√™ precisar. J√° deixamos algumas sugest√µes prontas mas fique a vontade para usar quaisquer outras desejadas :)
%pip install pandas
%pip install numpy
%pip install matplotlib
%pip install scikit-learn
#%pip install xgboost
#%pip install seaborn
import pandas as pd # Carregamento e manipula√ß√£o de dados
import matplotlib as plt # Visualiza√ß√£o
import numpy as np # lib para utiliza√ß√£o de opera√ß√µes numericas
from sklearn.preprocessing import OneHotEncoder # lib para transformar as vari√°veis categ√≥ricas em colunas bin√°rias que facilitam sua manipula√ß√£o
from sklearn.model_selection import train_test_split # lib para separa√ß√£o de de conjunto de dados entre treino e teste, com objetivo de prevenir overfitting
from sklearn.impute import SimpleImputer # Preven√ß√£o de dados nulos no modelo
from sklearn.compose import ColumnTransformer # lib que permite aplicar transforma√ß√µes diferentes para vari√°veis numericas e categoricas
from sklearn.pipeline import Pipeline # clean code
from sklearn.linear_model import LinearRegression # lib que implementa coeficientes para as vari√°veis, visando minimizar os erros entre os valores reais e os previstos
from sklearn.metrics import mean_absolute_error # Implementa√ß√£o do MAE para mostrar a varian√ßa dos dados previstos com os dados reais
from sklearn.metrics import mean_squared_error # Implementa√ß√£o do RMSE para mostrar a varian√ßa dos dados previstos com os dados reais
from sklearn.metrics import r2_score # Implementa√ß√£o do R¬≤ para mostrar a varian√ßa dos dados previstos com os dados reais
## üìÅ 1. Carregamento e limpeza dos dados
# remova os coment√°rios para ler o arquivo do github
# carregando o dataframe diretamente do github
# import pandas as pd
# file_path = "https://raw.githubusercontent.com/silva-raphael/ps-datascience-gft/refs/heads/main/filtered-enem-2019.csv"

# Realizando leitura do dataframe
df = pd.read_csv("https://raw.githubusercontent.com/silva-raphael/ps-datascience-gft/refs/heads/main/filtered-enem-2019.csv")


### Limpeza e tratamento
# Descartando a vari√°vel NU_ANO, visto que o dataset mant√©m essa informa√ß√£o est√°tica
df = df[[
    'NU_NOTA_CN',
    'NU_NOTA_CH',
    'NU_NOTA_LC',
    'NU_NOTA_MT',
    'TP_ESCOLA',
    'NU_NOTA_REDACAO',
    'TP_ST_CONCLUSAO'
]]

# Verifica√ß√£o de campos nulos, caso existam, eu devo substitui-los por outro valor para n√£o afetar no algoritmo
df.isnull().sum()

# Remo√ß√£o da NU_NOTA_REDACAO do dataframe, X recebendo o dataframe inteiro exceto NU_NOTA_REDACAO, e y contendo a totalidade da NU_NOTA_REDACAO
X = df.drop('NU_NOTA_REDACAO', axis=1)
y = df['NU_NOTA_REDACAO']
### Visualiza√ß√µes iniciais
# Classificando as vari√°veis num√©ricas e as categ√≥ricas
numericas = ['NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT']
categoricas = ['TP_ESCOLA', 'TP_ST_CONCLUSAO']

# O OneHotEncoder transforma as vari√°veis categ√≥ricas em colunas bin√°rias, visando evitar confus√µes no algoritmo, visto que essas vari√°veis vao divergir dos valores de nota que ser√£o pesados para prever o valor da nota da reda√ß√£o
cat_pipeline = Pipeline([
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

# O ColumnTransformer foi utilizado para aplicar transforma√ß√µes diferentes para cada tipo de vari√°vel, para prosseguir com o uso delas na pipeline
preprocessamento = ColumnTransformer([
    ('num', 'passthrough', numericas), # Passa as vari√°veis num√©ricas diretamente, da maneira que est√£o no dataframe
    ('cat', cat_pipeline, categoricas) # Passa as vari√°veis categoricas com o OneHotEncoder aplicado
])
## ü§ñ 2. An√°lise explorat√≥ria dos dados
# Cria√ß√†o da pipeline, usando os dados tratados anteriormente, e aplicando a equa√ß√£o de regress√†o linear
modelo = Pipeline([
    ('preprocessamento', preprocessamento), # Objeto definido anteriormente, contendo as colunas transformadas por tipo de vari√°vel
    ('regressor', LinearRegression()) # Aplica o modelo de regress√£o linear, usando a equa√ß√£o para chegar no valor previsto da NU_NOTA_REDACAO com base nos coeficientes num√©ricos (outras notas) + as vari√°veis categ√≥ricas
])
## üß™ 3. Constru√ß√£o do modelo (Treinamento e avalia√ß√£o)
# Aplica√ß√£o do train_test_split, que separa a massa em 20% (test_size=0.2) para testes e o restante para treinamento, visando treinar o modelo com uma fatia dos dados, e em seguida testar o desempenho do modelo com uma parte dos dados que ainda n√£o foi vista pelo modelo
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Achei bizarro o 42 sert√£o comum como divisor nas aulas que assisti e basicamente em toda aplica√ß√£o de treinamento, tive que pesquisar para entender por que tanta gente usa o 42 e n√£o me decepcionei

# Treinamento do modelo usando as vari√°veis de treinamento separadas anteriormente
modelo.fit(X_train, y_train) # o m√©todo .fit() ir√° treinar o modelo a prever a vari√°vel y (NU_NOTA_REDACAO) baseado nos coeficientes X previamente estabelecidos pelo LinearRegression()

# Agora que o treinamento est√° conclu√≠do, podemos prosseguir com a aplica√ß√£o das m√©tricas
y_pred = modelo.predict(X_test) # ir√° fazer previs√µes com os dados de teste que ainda n√£o foram vistos no treinamento do modelo.
## üß† 4. Avalia√ß√£o do modelo
# MAE
mae = mean_absolute_error(y_test, y_pred) # Aplica a metrica Erro M√©dio Absoluto, que visa contabilizar a variancia entre a previs√£o do modelo com os dados reais a partir dos erros absolutos
print(f"MAE: {mae:.2f}") # exibe o valor do erro m√©dio em pontos da reda√ß√£o

#RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred)) # Similar ao MAE, ele tamb√©m calcula a vari√¢ncia com base nos erros, por√©m consegue pegar os casos mais extremos (outliers) por elevar as m√©tricas ao quadrado, expondo os outliers, igual o MSE, por√©m ao aplicar a raiz quadrada √© poss√≠vel visualizar a vari√¢ncia na mesma escala que os dados originais
print(f"RMSE: {rmse:.2f}")

#R¬≤
r2 = r2_score(y_test, y_pred) # M√©trica que representa o percentual da vari√¢ncia dos dados em porcentagem
print(f"R¬≤: {r2:.2f}")
## üó£Ô∏è 5. Conclus√£o
Como deu para ver eu n√£o tenho muito conhecimento na ci√™ncia de dados, maioria dos meus coment√°rios estavam servindo como fixadores para eu entender melhor o que cada m√©todo estava fazendo e explicando qual a raz√£o de utilizar eles.

N√£o tenho o conhecimento necess√°rio ainda para entender como melhorar a vari√¢ncia das previs√µes sem ter uma massa de dados maior, por√©m estou com bastante interesse em continuar aprendendo mais sobre o assunto, que acabou me interessando bastante, mesmo sendo diferente da engenharia de dados que eu estava estudando previamente (como os dados estavam bem limpos n√£o teve muita coisa a ser cortada).
